{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Goal: Capturing images from the newspaper daily headlines displayed by the Newseum. May be a gray area, legally, as Newseum says I need to contact the newspaper publisher directly. But we'll see...\n",
    "\n",
    "http://www.newseum.org/todaysfrontpages/?tfp_show=all\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1) Downloading the newseum html that contains the links needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from urllib.request import urlopen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using url from the appropriate page on the newseum website (make sure show=all for all possible files), beautifulSoup is used to read the newseum page after downloading. The front page of each newspaper has several links associated, including a link to the paper's site and several image/pdfs kept at their own url. The pdf looks to be the highest res, so I'll grab those and convert to images later with ghostscript.\n",
    "\n",
    "The links for all these newspaper pdfs are in a script section of the html. I bet there is a nice, pretty way to use bs4 to get that. Instead, I bang it with a rock till I get what I want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26 583775\n"
     ]
    }
   ],
   "source": [
    "url='http://www.newseum.org/todaysfrontpages/?tfp_show=all' #today\n",
    "#url=\"http://www.newseum.org/todaysfrontpages/?tfp_display=archive-date&tfp_archive_id=100113&tfp_show=all\"#archive 2013 shutdown 1\n",
    "html=urlopen(url)\n",
    "#This next section navigates through the html and prints the date to help verify\n",
    "#the variable string replacement was successful. We'll get into using BeautifulSoup in the next section!\n",
    "#Fow now, just treat it asa a black box\n",
    "from bs4 import BeautifulSoup #black box\n",
    "soup=BeautifulSoup(html.read(),'html.parser') #black box\n",
    "soup2=soup.findAll('script')\n",
    "len(soup2)\n",
    "l=-1;\n",
    "loc=-1\n",
    "#longest script section is the one with all the newspaper front pages, so this give me that correct section.\n",
    "for x in range(0,len(soup2)):\n",
    "    if len(str(soup2[x]))>l:\n",
    "        l=len(str(soup2[x]))\n",
    "        loc=x\n",
    "print(loc,l)\n",
    "soup_string=str(soup2[loc])\n",
    "soup_str_split=soup_string.split(\",\")\n",
    "#getting the links for the pdfs:\n",
    "soup_pdf_links=[elm[6:].replace('\"','').replace(\"\\\\/\",\"/\") for elm in soup_str_split if elm.startswith('\"pdf')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "830\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['http://cdn.newseum.org/dfp/pdf9/AL_ACO.pdf',\n",
       " 'http://cdn.newseum.org/dfp/pdf9/AL_AS.pdf',\n",
       " 'http://cdn.newseum.org/dfp/pdf9/AL_BN.pdf',\n",
       " 'http://cdn.newseum.org/dfp/pdf9/AL_DD.pdf',\n",
       " 'http://cdn.newseum.org/dfp/pdf9/AL_DE.pdf',\n",
       " 'http://cdn.newseum.org/dfp/pdf9/AL_EL.pdf',\n",
       " 'http://cdn.newseum.org/dfp/pdf9/AL_TD.pdf',\n",
       " 'http://cdn.newseum.org/dfp/pdf9/AL_GT.pdf',\n",
       " 'http://cdn.newseum.org/dfp/pdf9/AL_HT.pdf',\n",
       " 'http://cdn.newseum.org/dfp/pdf9/AL_MR.pdf']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(soup_pdf_links))\n",
    "soup_pdf_links[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I want to timestamp these. I'll be doing that in a few different ways. First, pdfs for each day will be in their own folder. Second, the filename for each pdf will contain a timestamp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from time import gmtime, strftime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'9'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "strftime(\"%d\").replace('0','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'20180209_AL_ACO.pdf'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t=strftime(\"%Y%m%d\") #today\n",
    "#t=\"20131017\"#archive\n",
    "url=soup_pdf_links[0]\n",
    "n=url.split('/pdf%s/'%(strftime(\"%d\").replace('0','')))[1]#today\n",
    "#n=url.split('/pdf/')[1]#archive\n",
    "t+\"_\"+n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Direct save of those pdf to a local folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 AL_ACO.pdf\n",
      "10 AL_MA.pdf\n",
      "20 AZ_TNH.pdf\n",
      "30 CA_TU.pdf\n",
      "40 CA_NVR.pdf\n",
      "50 CA_PE.pdf\n",
      "60 CA_SCS.pdf\n",
      "70 CA_EBT.pdf\n",
      "80 CO_FCC.pdf\n",
      "90 CT_GT.pdf\n",
      "100 FL_CCC.pdf\n",
      "110 FL_NDN.pdf\n",
      "120 FL_VDS.pdf\n",
      "130 GA_TT.pdf\n",
      "140 ID_TN.pdf\n",
      "150 IL_JG.pdf\n",
      "160 IN_MC.pdf\n",
      "170 IN_SP.pdf\n",
      "180 IA_QCT.pdf\n",
      "190 KY_TG.pdf\n",
      "200 LA_DA.pdf\n",
      "210 ME_PPH.pdf\n",
      "220 MA_BG.pdf\n",
      "230 MA_DNN.pdf\n",
      "240 MI_DN.pdf\n",
      "250 MI_TH.pdf\n",
      "260 MN_WDN.pdf\n",
      "270 MO_NL.pdf\n",
      "280 NE_GII.pdf\n",
      "290 NH_PH.pdf\n",
      "300 NJ_DR.pdf\n",
      "310 NM_DH.pdf\n",
      "320 NY_PSB.pdf\n",
      "330 NY_LUSJ.pdf\n",
      "340 WSJ.pdf\n",
      "350 NY_JN.pdf\n",
      "360 NC_MN.pdf\n",
      "370 ND_GFH.pdf\n",
      "380 OH_TR.pdf\n",
      "390 OH_SNS.pdf\n",
      "400 OR_HN.pdf\n",
      "410 PA_TI.pdf\n",
      "420 PA_LDN.pdf\n",
      "430 PA_MER.pdf\n",
      "440 PA_CV.pdf\n",
      "450 SC_PC.pdf\n",
      "460 SD_SFAL.pdf\n",
      "470 TX_ARN.pdf\n",
      "480 TX_HC.pdf\n",
      "490 TX_VA.pdf\n",
      "500 VT_RH.pdf\n",
      "510 VA_VP.pdf\n",
      "520 WA_ST.pdf\n",
      "530 WI_BNR.pdf\n",
      "540 WI_MNH.pdf\n",
      "550 USA_SSM.pdf\n",
      "560 AUS_WA.pdf\n",
      "570 BER_RG.pdf\n",
      "580 BRA^SP_METCA.pdf\n",
      "590 BRA^RS_JDG.pdf\n",
      "600 BRA_DG.pdf\n",
      "610 BRA_JVS.pdf\n",
      "620 BRA_AGAZ.pdf\n",
      "630 CAN_EDM.pdf\n",
      "640 CAN_OC.pdf\n",
      "650 CAN_TDN.pdf\n",
      "660 CHI_PD.pdf\n",
      "670 COL_EC.pdf\n",
      "680 FRA_CP.pdf\n",
      "690 GER_RP.pdf\n",
      "700 HUN_HBN.pdf\n",
      "710 IND_AND.pdf\n",
      "720 IND_PRAHS.pdf\n",
      "730 JAM_JO.pdf\n",
      "740 MAL_STAR.pdf\n",
      "750 MEX_EHDS.pdf\n",
      "760 NET_AD2.pdf\n",
      "770 PAR_UH.pdf\n",
      "780 ROM_JA.pdf\n",
      "790 SLK_SME.pdf\n",
      "800 SPA_PUNTB.pdf\n",
      "810 SWE_DN.pdf\n",
      "820 TUR_TICAR.pdf\n"
     ]
    }
   ],
   "source": [
    "from time import gmtime, strftime\n",
    "import time\n",
    "import urllib.request\n",
    "import random\n",
    "#t=strftime(\"%Y%m%d\")\n",
    "#t=\"20131002\"#archive\n",
    "for x in range(0,len(soup_pdf_links)):\n",
    "    url=soup_pdf_links[x]\n",
    "    n=url.split('/pdf%s/'%(strftime(\"%d\").replace('0','')))[1]#today\n",
    "    #n=url.split('/pdf/')[1]#archive\n",
    "    label=t+\"_\"+n\n",
    "    urllib.request.urlretrieve(url, 'C:/path/to/save/folder/%s/%s'%(t,label))\n",
    "    wait=random.randrange(10,30,1)*.1;\n",
    "    time.sleep(wait) #to not abuse the newseum's servers, adding a delay. Making it a bit random to play with mimicing a human.\n",
    "    if x%10==0:\n",
    "        print(x,n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
